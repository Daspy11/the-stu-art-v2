---
series: 
aliases:
  - Large language models
---

# AI Predictions
#essay 


We all like to laugh at the silly, supposedly prestigious goons who said very foolish things about the development of technology. The old technology leaders of the past who said that the internet and personal computing were nonces that would never find adoption.

Conversely, I was deeply embedded in the venture capital and startup scene during the crypto boom, and prior to the fall of the Terra ecosystem, I remember being sat there thinking *have I lost my mind?* 

It seemed all of the minds I respected the most were seemingly tremendously bought-in to this new set of technology that seemed to me so nakedly without a future. Even Y Combinator, an institution I respect deeply, backed StableGains, a company which offered a 15% annual return on investment by relying on StableCoin staking mechanisms. Am I crazy? Doesn't this just not work? Doesn't it rely on an infinite money funnel that cannot continue? The same people who seemed very excited by this were the same people who'd backed a lot of great companies I got a lot of value out of when others thought it was crazy. I wondered if I was just out of touch.

All over the place, the biggest minds were dropping billions of dollars on ventures that just... didn't make sense to me. 

And then Terra, Three Arrows, and FTX happened, and I felt a little validated. NFTs increasingly look like they'll be remembered like Dutch tulip mania. 

The fall of the metaverse was similar. It just never made sense to me.



But that was then.



I see some people talking the same way about large language models as I have been about cryptocurrency. 



I see the artists clutching their copyright law. 



I see the old-timey Linux sysadmin types grumbling in a corner about how this isn't anything new, and that the technology will never progress beyond barely-competent bullshit generators.



I see the AI doom faction saying lots of things about doom, and their adversaries saying lots of things about regulatory capture. I'm not sure I identify with either of them.



Some more nuanced commenters like this one from Hacker News offer a somewhat plausible argument for diminishing returns:

> I don't believe LLM's will ever become AGI, partly because I don't believe that training on the outputs of human intelligence (i.e. human-written text) will ever produce something equivalent to human intelligence.
> 
> You can't model and predict the weather just by training on the outputs of the weather system (whether it rained today, whether it was cloudy yesterday, and so on). You have to train on the inputs (air currents, warm fronts, etc.)
>
> You can't model and predict the stock market just by training on the outputs of stock trading decisions (the high today, the low yesterday). You have to train on the inputs (company fundamentals, earnings, market sentiments in the news, etc.)
> 
> I similarly think you have to train on the inputs of human decision-making to create something which can model human decision-making. What are those inputs? We don't fully know, but it is probably some subset of the spatial and auditory information we take in from birth until the point we become mature, with "feeling" and "emotion" as a reward function (seek joy, avoid pain, seek warmth, avoid hunger, seek victory, avoid embarrassment and defeat, etc.)
> 
> Language models are always playing catch-up because they don't actually understand how the world works. The cracks through which we will typically notice that they don't, in the context of the tasks typically asked of them (summarize this article, write a short story), will gradually get smaller over time (due to RLHF), but the fundamental weakness will always remain.
> 
> - [wavemode](https://news.ycombinator.com/user?id=wavemode)


I'm going to take a big bet against everyone who's even a little bit cynical.


In the future, I can either look back at this and laugh at my naivety, or I can wave it in everyone's face.


I think change is coming, and it's going to be big. Maybe bigger in terms of effect on people than the internet. Big in the way of civilisation being almost unrecognisable soon. 

While there's obviously more science to come in terms of LLM capabilities, the process which it's clear to me we've barely started is discovery of new interaction modalities. 

When Apple invented the mouse, they had to educate consumers on how to use it. They knew the ceiling of interaction quality was going to be much higher than terminal or keyboard-based navigation because of the intuitiveness. They won again with the touchscreen. Both interfaces are now ubiquitous. 

In both cases though, although a new kind of input had been created, it took much longer for optimal ergonomics to develop. I think it's unlikely a mouse will ever release that is a substantial advance on technology invented in the early 2010s - the Logitech G502 comes to mind. The same is true of touchscreens - there's probably just not that much more to do. I think if you held an early mouse from any manufacturer, you'd surely feel that this is not the final form. Not only were there improvements to make, there were clearly no major technological barriers to those gains either.

Video games are an example of the opposite - technological limitations on the power of home computers are a hard limitation on the ergonomics of games. Selective rendering, zoning and a slew of optimisations get beautiful games to run, but if we could suddenly 10x the power of home computers, we'd likely see an overnight spike in the beauty of new video games.

If you're a video game designer developing a game with cutting edge graphics, you need to take bets on the kind of fidelity you'll be able to achieve on consumer grade hardware of the next few years if you want to truly take advantage of the technology on offer. This introduces some uncertainty, but it's still a pretty reasonable bet to make that everyone's computer will be a Little Bit More Powerful in a few years than they are now, and there isn't much variance to worry about there either.

While some decisions you make can be easily scaled, others can't. Graphical effects can be enabled or disabled programmatically, but CPU load in the form of interactions and game logic can't easily be. 

Eventually, large language models will be ubiquitous, like the mouse or touchscreen. Part of my wager is that they're going to go further than ubiquitous and become the fabric on which society is built - like the internet. The internet is much more important than just a mere ubiquitous tool interacted with by billions - it's not really even possible to build a society that works without it. 

ChatGPT is bricky as hell. I think ChatGPT is like the skeuomorphism of the early 2000s. In the days where digitisation was new, a common philosophy amongst designers was to rely on making digital technology look similar to tools we use in the real world. The early concept of "clickability" (being the degree to which we guess whether an element is interactable) was simply to model the resulting effect from the visual appearance of the object. The notepad looked like a notepad, and from that, the visual cue is that it functions like a notepad and you should use it as such.

As I write this in Obsidian, I'm surrounded by a litany of design widgets that even the best designers of the early 2000s couldn't have conceived of. 

A lot of people more mathematically inclined than me are going to do a lot of clever work on advancing AI models, but I think there's work to be done advancing new LLM interaction modalities that go beyond the skeuomorphic approach of replicating a conversational experience. Even in the case of extremely rapid progressions in AI capability, subjective evaluations still need humans in the loop to be convinced and to adopt. 

That also means I'm placing a bet against any system whose main interaction modality is skeuomorphic of human interaction. I've seen a variety of hucksters trying to bundle up AI personas as employee replacements, or chat interfaces for code or education, or as customer support representatives. These are crude, primitive tools which will not be looked back on fondly. Don't get me wrong, language models *will* replace educators and call centres, but they won't do so as feeble imitations. Even near-perfect replication isn't enough to outdo the uncanny valley. 

These tools will continue to have relevance as roleplaying tools for leisure and may have niche pedagogic utility as a way to introduce fun into education, but it's not how the future is going to look.

Smartphones became a physical extension of our bodies that we're anxious and often unable to function without. They give us incredible superpowers. Perfect knowledge of direction and the logistics required to get to any place. Instant communication with anyone, anywhere in the world. Any kind of information, immediately. I sound like something out of a late 2000s magazine article in some long-forgotten churnalist piece, but let's not underestimate the automaticity with which smartphones extended us.

There have been some hamfisted attempts to do the same with glasses, headsets, or most recently shitty badges, but they don't *quite work* for reasons that are nuanced and hard to anticipate ahead of time. It was famously cited that it felt rude to be talking to a Google glass user as they looked up at their notifications. I think this glaring social deficit which killed it out of the gate is hard to anticipate, even if you're a motivated and capable interaction designer. I'll reserve judgement on Apple's latest attempts for now. 

The closest thing to language models becoming an extension of the body so far I think has been felt by software engineers interacting with LLM-powered autocomplete for code. There are many times where, like magic, AI just wrote a whole block of code I was about to bash out piece by piece. That didn't feel like I was using a tool, it felt like my own capabilities were being extended. There's no doubt that autocomplete is one of the modalities which will win, but there are others that are yet to be uncovered. There are a few I want to take a bet on.

I think voice commands are coming back big-time. Alexa and Google home fell off because they couldn't provide enough utility to do more than just play music and so were hard to monetise. The programmed-in example capabilities were much less organic in person. Right now, I think the biggest limitation to the extension-of-body is interoperability. In the future, I think all systems will have a separate kind of documentation targeted at large language models which defines its capabilities and the standard input model to achieve it. Like how JSON became the backbone for REST APIs. 

But these guesses are ephemeral and I'm not married to them. What I think I know is:

1. Artists will lose the intellectual property war
2. In fiction, the capabilities of large language models will overtake that of human creatives very shortly. The author profession as we know it is dead. Initially, this will look like a massive wave of democratisation as the work requirement to produce books decreases and ultimately surpasses the best human writers in terms of appeal. Eventually, that democratisation will be captured at scale by a few large companies that provide personalised fiction, likely with interactive elements, at scale.
3. In the same vein, the days of static fiction are numbered. The video game genre will grow to consume both TV shows and fiction books, both of which will become relics, like silent movies.
4. The video games of the 2030s and 2040s are going to be unimaginably cool. Not only will large, open-world, triple A titles continue to grow in scale, I think the best gains to be had are in the micro - every guard and fisherman and mercenary has their own story. 
6. Special pleading about how language models just predict and therefore can't be intelligent will be killed in the same way that the scientific method killed God - it doesn't defeat him, but pushes back the scope of his capabilities tremendously. 
